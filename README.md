# NLP-Projects
## Assignment 1 
covers 3 parts:

**1. Language models**:
  * Use n-gram models to generate a language model,
  * Compare different language models using perplexity
  * Compare different smoothing methods for n-gram distribution estimation
  * Compare word-based and character-based language models
  * Compare the expressive power of n-gram models and neural-network language models.
 
**2. Linear regression in a probabilistic model and regularization**:
  * Manipulate basic statistical distributions (Normal, Multinomial), expectations, sampling and estimation.
  * Perform regression using a synthetic dataset.
  * Develop an intuition of how regularization helps overcome overfitting in a simple case.
 
**3. Text classification using a PyTorch character RNN**:
  * Experiment with neural networks - encode text into vectors and run training and evaluation on a small dataset.

## Assignment 2 
covers the topic of:
1. **Document classification** -Apply feature-based supervised machine learning methods for document and token classification.
2. **Sequence classification** - Investigate algorithms for sequence classification: HMMs and CRFs.
3. **Named Entity Recognition and word embeddings-**:
    * Learn the specific tasks of word classification, named entity recognition and document classification.
    * Use pre-trained word embeddings and measure whether they help for the task of NER.

## Assignment 3: 
This assignment covers the topic of syntactic parsing
  * Designing CFGs for NLP
  * Learning a PCFG from a Treebank - with and without Chomskey Normal form
  * Building and Evaluating a Simple PCFG Parser
